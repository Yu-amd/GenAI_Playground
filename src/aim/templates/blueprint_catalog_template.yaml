# Blueprint Catalog Template
# This template defines the structure for a single YAML file that contains all blueprints
# in the Inference Hub catalog. Each blueprint is defined as an entry in the blueprints array.

catalog_metadata:
  version: "1.0.0"
  last_updated: "2024-12-19"
  total_blueprints: 0
  categories:
    - Conversational AI
    - Computer Vision
    - Multimodal
    - Code Generation
    - Document Processing
    - Audio Processing
    - Recommendation Systems
    - Search & Retrieval

blueprints:
  # Template for a single blueprint entry
  - blueprint_id: <unique-blueprint-id>  # e.g., chatqna, imagegen, docsummarizer
    name: <Blueprint Display Name>  # e.g., ChatQnA, Image Generator, Document Summarizer
    category: <Category>  # e.g., Conversational AI, Computer Vision, Document Processing
    complexity: <Complexity Level>  # e.g., Beginner, Intermediate, Advanced
    description: >
      Comprehensive description of what the blueprint does, its architecture,
      and key capabilities. This should be detailed enough for users to understand
      the blueprint's purpose and functionality.
    shortDescription: >
      Brief one-line description that appears in catalog listings and cards.
      Should be concise but informative.
    logo: <logo-filename>  # e.g., bp_chatqna.png, bp_imagegen.png
    readiness_level: <Readiness>  # e.g., Production-Ready, Tech Preview, Day-0 Available
    status_badges:
      - <Badge1>  # e.g., Featured, New, Production-Ready, RAG
      - <Badge2>  # e.g., Popular, Trending, Beta
    tags:
      - <Tag1>  # e.g., RAG, Chatbot, Knowledge Base
      - <Tag2>  # e.g., Vector Search, Document Processing
      - <Tag3>  # e.g., Multimodal, API
    status: <Status>  # e.g., Production Ready, Beta, Alpha
    endpoint: <https://api.inference-hub.com/v1/blueprints/<blueprint-id>>
    
    demo_assets:
      notebook: <https://github.com/inference-hub/blueprints/<blueprint-id>-demo.ipynb>
      demo_link: <https://playground.inference-hub.com/blueprints/<blueprint-id>>
    
    aim_recipes:
      - name: <Hardware> <Precision>  # e.g., MI300X FP16, MI250 FP16
        hardware: <Hardware>  # e.g., MI300X, MI250, MI100
        precision: <Precision>  # e.g., fp16, fp8, int8
        recipe_file: configs/<blueprint-id>-<hardware>-<precision>.yaml
    
    api_examples:
      python: |
        import requests

        headers = {
            "Authorization": "Bearer YOUR_API_KEY",
            "Content-Type": "application/json"
        }

        payload = {
            "blueprint": "<blueprint_id>",
            "query": "<example_query>",
            "parameters": {
                # Add blueprint-specific parameters here
            },
            "stream": False
        }

        response = requests.post("<endpoint>", headers=headers, json=payload)
        print(response.json())
      
      typescript: |
        const response = await fetch("<endpoint>", {
          method: "POST",
          headers: {
            "Authorization": "Bearer YOUR_API_KEY",
            "Content-Type": "application/json"
          },
          body: JSON.stringify({
            blueprint: "<blueprint_id>",
            query: "<example_query>",
            parameters: {
              // Add blueprint-specific parameters here
            },
            stream: false
          })
        });

        const data = await response.json();
        console.log(data.response);
      
      shell: |
        curl -X POST <endpoint> \
          -H "Authorization: Bearer YOUR_API_KEY" \
          -H "Content-Type: application/json" \
          -d '{
            "blueprint": "<blueprint_id>",
            "query": "<example_query>",
            "parameters": {
              // Add blueprint-specific parameters here
            },
            "stream": false
          }'
      
      rust: |
        use axum::{
            extract::Json,
            http::StatusCode,
            response::sse::{Event, Sse},
            routing::post,
            Router,
        };
        use serde::{Deserialize, Serialize};
        use std::convert::Infallible;
        use tokio_stream::wrappers::ReceiverStream;

        #[derive(Deserialize)]
        struct BlueprintRequest {
            blueprint: String,
            query: String,
            parameters: serde_json::Value,
            stream: bool,
        }

        async fn blueprint_completion(Json(payload): Json<BlueprintRequest>) -> Sse<impl Stream<Item = Result<Event, Infallible>>> {
            let (tx, rx) = tokio::sync::mpsc::channel(100);
            
            tokio::spawn(async move {
                // Simulate blueprint response
                let response = format!("Blueprint response for: {}", payload.blueprint);
                for chunk in response.chars() {
                    let event = Event::default().data(chunk.to_string());
                    let _ = tx.send(Ok(event)).await;
                    tokio::time::sleep(tokio::time::Duration::from_millis(50)).await;
                }
            });
            
            Sse::new(ReceiverStream::new(rx))
        }

        #[tokio::main]
        async fn main() {
            let app = Router::new()
                .route("/blueprints/<blueprint_id>", post(blueprint_completion));
            
            axum::Server::bind(&"0.0.0.0:3000".parse().unwrap())
                .serve(app.into_make_service())
                .await
                .unwrap();
        }
      
      go: |
        package main

        import (
            "bytes"
            "fmt"
            "io/ioutil"
            "net/http"
        )

        func main() {
            jsonStr := []byte(`{
                "blueprint": "<blueprint_id>",
                "query": "<example_query>",
                "parameters": {
                    // Add blueprint-specific parameters here
                },
                "stream": false
            }`)

            req, _ := http.NewRequest("POST", "<endpoint>", bytes.NewBuffer(jsonStr))
            req.Header.Set("Authorization", "Bearer YOUR_API_KEY")
            req.Header.Set("Content-Type", "application/json")

            client := &http.Client{}
            resp, _ := client.Do(req)
            body, _ := ioutil.ReadAll(resp.Body)
            fmt.Println(string(body))
        }
    
    blueprint_card:
      overview: >
        Detailed overview of the blueprint, its architecture, and how it works.
        This should provide users with a comprehensive understanding of the
        blueprint's capabilities and implementation approach.
      intended_use:
        - <Use case 1>  # e.g., Customer support chatbots
        - <Use case 2>  # e.g., Knowledge base querying
        - <Use case 3>  # e.g., Document-based Q&A systems
      limitations:
        - <Limitation 1>  # e.g., Requires well-structured knowledge base
        - <Limitation 2>  # e.g., Response quality depends on retrieval accuracy
        - <Limitation 3>  # e.g., May not handle complex multi-step reasoning
      architecture: >
        Brief description of the blueprint's architecture, including key
        components and how they interact.
      evaluation:
        - <Metric 1>: <Value>  # e.g., Response accuracy: 92%
        - <Metric 2>: <Value>  # e.g., Retrieval precision: 89%
        - <Metric 3>: <Value>  # e.g., User satisfaction: 4.2/5
        - <Metric 4>: <Value>  # e.g., Response time: <2 seconds
      known_issues:
        - <Issue 1>  # e.g., Occasional hallucination when knowledge base is incomplete
        - <Issue 2>  # e.g., Performance degradation with very large knowledge bases
        - <Issue 3>  # e.g., Limited context window for complex queries
      references:
        - <Reference URL 1>  # e.g., https://arxiv.org/abs/2005.11401
        - <Reference URL 2>  # e.g., https://github.com/inference-hub/blueprints/<blueprint_id>
    
    microservices:
      models:
        - name: <Model Name>  # e.g., Qwen2 7B, Embedding Model
          logo: <model-logo-path>  # e.g., /src/assets/models/model_Qwen2-7B.png
          tags:
            - <Model Tag 1>  # e.g., Text Generation, RAG
            - <Model Tag 2>  # e.g., Inference, Embeddings
        - name: <Another Model Name>
          logo: <another-model-logo-path>
          tags:
            - <Another Model Tag 1>
            - <Another Model Tag 2>
      functional:
        - name: <Service Name>  # e.g., Retriever, Reranking
          description: <Service Description>  # e.g., Vector-based document retrieval service
          tags:
            - <Service Tag 1>  # e.g., RAG, Vector Search
            - <Service Tag 2>  # e.g., Document Processing, Ranking

  # Example blueprint entry (ChatQnA)
  - blueprint_id: chatqna
    name: ChatQnA
    category: Conversational AI
    complexity: Intermediate
    description: Chatbot application based on Retrieval Augmented Generation architecture.
    shortDescription: RAG-based chatbot for knowledge base interactions with advanced retrieval capabilities.
    logo: bp_chatqna.png
    readiness_level: Production-Ready
    status_badges:
      - Featured
      - Production-Ready
      - RAG
    tags:
      - RAG
      - Chatbot
      - Knowledge Base
      - Vector Search
      - Document Processing
    status: Production Ready
    endpoint: https://api.inference-hub.com/v1/blueprints/chatqna
    demo_assets:
      notebook: https://github.com/inference-hub/blueprints/chatqna-demo.ipynb
      demo_link: https://playground.inference-hub.com/blueprints/chatqna
    aim_recipes:
      - name: MI300X FP16
        hardware: MI300X
        precision: fp16
        recipe_file: configs/chatqna-mi300x-fp16.yaml
      - name: MI250 FP16
        hardware: MI250
        precision: fp16
        recipe_file: configs/chatqna-mi250-fp16.yaml
    api_examples:
      python: |
        import requests

        headers = {
            "Authorization": "Bearer YOUR_API_KEY",
            "Content-Type": "application/json"
        }

        payload = {
            "blueprint": "chatqna",
            "query": "What is the OPEA Framework?",
            "knowledge_base": "opea_docs",
            "stream": False
        }

        response = requests.post("https://api.inference-hub.com/v1/blueprints/chatqna", headers=headers, json=payload)
        print(response.json())
      typescript: |
        const response = await fetch("https://api.inference-hub.com/v1/blueprints/chatqna", {
          method: "POST",
          headers: {
            "Authorization": "Bearer YOUR_API_KEY",
            "Content-Type": "application/json"
          },
          body: JSON.stringify({
            blueprint: "chatqna",
            query: "What is the OPEA Framework?",
            knowledge_base: "opea_docs",
            stream: false
          })
        });

        const data = await response.json();
        console.log(data.response);
      shell: |
        curl -X POST https://api.inference-hub.com/v1/blueprints/chatqna \
          -H "Authorization: Bearer YOUR_API_KEY" \
          -H "Content-Type: application/json" \
          -d '{
            "blueprint": "chatqna",
            "query": "What is the OPEA Framework?",
            "knowledge_base": "opea_docs",
            "stream": false
          }'
      rust: |
        use axum::{
            extract::Json,
            http::StatusCode,
            response::sse::{Event, Sse},
            routing::post,
            Router,
        };
        use serde::{Deserialize, Serialize};
        use std::convert::Infallible;
        use tokio_stream::wrappers::ReceiverStream;

        #[derive(Deserialize)]
        struct ChatQnARequest {
            blueprint: String,
            query: String,
            knowledge_base: String,
            stream: bool,
        }

        async fn chatqna_completion(Json(payload): Json<ChatQnARequest>) -> Sse<impl Stream<Item = Result<Event, Infallible>>> {
            let (tx, rx) = tokio::sync::mpsc::channel(100);
            
            tokio::spawn(async move {
                // Simulate RAG response
                let response = format!("RAG response for query: {}", payload.query);
                for chunk in response.chars() {
                    let event = Event::default().data(chunk.to_string());
                    let _ = tx.send(Ok(event)).await;
                    tokio::time::sleep(tokio::time::Duration::from_millis(50)).await;
                }
            });
            
            Sse::new(ReceiverStream::new(rx))
        }

        #[tokio::main]
        async fn main() {
            let app = Router::new()
                .route("/blueprints/chatqna", post(chatqna_completion));
            
            axum::Server::bind(&"0.0.0.0:3000".parse().unwrap())
                .serve(app.into_make_service())
                .await
                .unwrap();
        }
      go: |
        package main

        import (
            "bytes"
            "fmt"
            "io/ioutil"
            "net/http"
        )

        func main() {
            jsonStr := []byte(`{
                "blueprint": "chatqna",
                "query": "What is the OPEA Framework?",
                "knowledge_base": "opea_docs",
                "stream": false
            }`)

            req, _ := http.NewRequest("POST", "https://api.inference-hub.com/v1/blueprints/chatqna", bytes.NewBuffer(jsonStr))
            req.Header.Set("Authorization", "Bearer YOUR_API_KEY")
            req.Header.Set("Content-Type", "application/json")

            client := &http.Client{}
            resp, _ := client.Do(req)
            body, _ := ioutil.ReadAll(resp.Body)
            fmt.Println(string(body))
        }
    blueprint_card:
      overview: ChatQnA is a Retrieval Augmented Generation (RAG) based chatbot that combines large language models with external knowledge bases to provide accurate, context-aware responses.
      intended_use:
        - Customer support chatbots
        - Knowledge base querying
        - Document-based Q&A systems
        - Technical support automation
      limitations:
        - Requires well-structured knowledge base
        - Response quality depends on retrieval accuracy
        - May not handle complex multi-step reasoning
        - Limited to information in the knowledge base
      architecture: RAG pipeline with vector database, embedding model, and LLM for generation
      evaluation:
        - Response accuracy: 92%
        - Retrieval precision: 89%
        - User satisfaction: 4.2/5
        - Response time: <2 seconds
      known_issues:
        - Occasional hallucination when knowledge base is incomplete
        - Performance degradation with very large knowledge bases
        - Limited context window for complex queries
      references:
        - https://arxiv.org/abs/2005.11401
        - https://github.com/inference-hub/blueprints/chatqna
    microservices:
      models:
        - name: Qwen2 7B
          logo: /src/assets/models/model_Qwen2-7B.png
          tags:
            - Text Generation
            - RAG
            - Inference
        - name: Embedding Model
          logo: /src/assets/models/model_embedding.png
          tags:
            - Embeddings
            - Vector Search
      functional:
        - name: Retriever
          description: Vector-based document retrieval service
          tags:
            - RAG
            - Vector Search
            - Document Processing
        - name: Reranking
          description: Re-ranks retrieved documents for better relevance
          tags:
            - RAG
            - Document Ranking
            - Relevance

  # Add more blueprint entries here following the same structure
  # - blueprint_id: imagegen
  #   name: Image Generator
  #   category: Computer Vision
  #   ...

# Usage Instructions:
# 1. Replace all placeholder values (enclosed in <>) with actual data
# 2. Add new blueprint entries by copying the template structure
# 3. Update catalog_metadata.total_blueprints count
# 4. Ensure all blueprint_ids are unique
# 5. Validate YAML syntax before committing
# 6. Update the blueprint loader to read from this catalog file 