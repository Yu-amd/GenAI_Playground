model_id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
name: LLaMA 4 Maverick 17B 128E Instruct FP8
builder: Meta AI
family: Llama
size: 17B
huggingface_id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
description: 'LLaMA 4 Maverick is a 17B parameter model with FP8 precision and 128K
  context length.  It''s designed for high-performance inference with reduced memory
  usage through FP8 quantization  while maintaining excellent instruction-following
  capabilities and extended context processing.

  '
logo: model_llama4_maverick.png
readiness_level: Production-Ready
status_badges:
- FP8
- FlashAttention
- Featured
tags:
- Text Generation
- Multilingual
- Instruction-Tuned
- vLLM-Compatible
- sglang-Compatible
license: Meta RAIL
endpoint: https://api.inference-hub.com/v1/chat/completions
demo_assets:
  notebook: https://github.com/inference-hub/notebooks/llama-4-maverick-17b-demo.ipynb
  demo_link: https://playground.inference-hub.com/models/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
aim_recipes:
- name: MI300X FP8
  hardware: MI300X
  precision: fp8
  recipe_file: configs/llama-4-maverick-17b-mi300x-fp8.yaml
- name: MI250 FP8
  hardware: MI250
  precision: fp8
  recipe_file: configs/llama-4-maverick-17b-mi250-fp8.yaml
api_examples:
  python: "import requests\n\nheaders = {\n    \"Authorization\": \"Bearer YOUR_API_KEY\"\
    ,\n    \"Content-Type\": \"application/json\"\n}\n\npayload = {\n    \"model\"\
    : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",\n    \"messages\": [{\"\
    role\": \"user\", \"content\": \"Hello\"}],\n    \"stream\": False\n}\n\nresponse\
    \ = requests.post(\"https://api.inference-hub.com/v1/chat/completions\", headers=headers,\
    \ json=payload)\nprint(response.json())\n"
  shell: "curl -X POST https://api.inference-hub.com/v1/chat/completions \\\n  -H\
    \ \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\"\
    \ \\\n  -d '{\n    \"model\": \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\"\
    ,\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n    \"stream\"\
    : false\n  }'\n"
  go: "package main\n\nimport (\n    \"bytes\"\n    \"fmt\"\n    \"io/ioutil\"\n \
    \   \"net/http\"\n)\n\nfunc main() {\n    jsonStr := []byte(`{\n        \"model\"\
    : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",\n        \"messages\"\
    : [{\"role\": \"user\", \"content\": \"Hello\"}],\n        \"stream\": false\n\
    \    }`)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.inference-hub.com/v1/chat/completions\"\
    , bytes.NewBuffer(jsonStr))\n    req.Header.Set(\"Authorization\", \"Bearer YOUR_API_KEY\"\
    )\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client :=\
    \ &http.Client{}\n    resp, _ := client.Do(req)\n    body, _ := ioutil.ReadAll(resp.Body)\n\
    \    fmt.Println(string(body))\n}\n"
  typescript: "const response = await fetch(\"https://api.inference-hub.com/v1/chat/completions\"\
    , {\n  method: \"POST\",\n  headers: {\n    \"Authorization\": \"Bearer YOUR_API_KEY\"\
    ,\n    \"Content-Type\": \"application/json\"\n  },\n  body: JSON.stringify({\n\
    \    model: \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",\n    messages:\
    \ [{ role: \"user\", content: \"Hello\" }],\n    stream: false\n  })\n});\n\n\
    const data = await response.json();\nconsole.log(data.choices[0].message.content);\n"
  rust: "use axum::{\n    extract::Json,\n    http::StatusCode,\n    response::sse::{Event,\
    \ Sse},\n    routing::post,\n    Router,\n};\nuse serde::{Deserialize, Serialize};\n\
    use std::convert::Infallible;\nuse tokio_stream::wrappers::ReceiverStream;\n\n\
    #[derive(Deserialize)]\nstruct ChatRequest {\n    model: String,\n    messages:\
    \ Vec<Message>,\n    temperature: f32,\n    max_tokens: u32,\n    top_p: f32,\n\
    \    stream: bool,\n}\n\n#[derive(Serialize, Deserialize)]\nstruct Message {\n\
    \    role: String,\n    content: String,\n}\n\nasync fn chat_completion(Json(payload):\
    \ Json<ChatRequest>) -> Sse<impl Stream<Item = Result<Event, Infallible>>> {\n\
    \    let (tx, rx) = tokio::sync::mpsc::channel(100);\n    \n    tokio::spawn(async\
    \ move {\n        // Simulate streaming response\n        let response = format!(\"\
    Response for model: {}\", payload.model);\n        for chunk in response.chars()\
    \ {\n            let event = Event::default().data(chunk.to_string());\n     \
    \       let _ = tx.send(Ok(event)).await;\n            tokio::time::sleep(tokio::time::Duration::from_millis(50)).await;\n\
    \        }\n    });\n    \n    Sse::new(ReceiverStream::new(rx))\n}\n\n#[tokio::main]\n\
    async fn main() {\n    let app = Router::new()\n        .route(\"/chat/completions\"\
    , post(chat_completion));\n    \n    axum::Server::bind(&\"0.0.0.0:3000\".parse().unwrap())\n\
    \        .serve(app.into_make_service())\n        .await\n        .unwrap();\n\
    }"
model_card:
  overview: ''
  intended_use: []
  limitations: []
  training_data: ''
  evaluation: []
  known_issues: []
  references: []
