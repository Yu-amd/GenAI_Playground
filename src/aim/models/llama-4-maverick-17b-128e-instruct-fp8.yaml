model_id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
name: LLaMA 4 Maverick 17B 128E Instruct FP8
builder: Meta AI
family: Llama
size: 17B
huggingface_id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8

description: >
  LLaMA 4 Maverick is a 17B parameter model with FP8 precision and 128K context length. 
  It's designed for high-performance inference with reduced memory usage through FP8 quantization 
  while maintaining excellent instruction-following capabilities and extended context processing.

logo: model_llama4_maverick.png

readiness_level: Production-Ready
status_badges:
  - FP8
  - FlashAttention
  - Featured

tags:
  - Text Generation
  - Multilingual
  - Instruction-Tuned
  - vLLM-Compatible
  - sglang-Compatible

license: Meta RAIL

endpoint: https://api.inference-hub.com/v1/chat/completions

demo_assets:
  notebook: https://github.com/inference-hub/notebooks/llama-4-maverick-17b-demo.ipynb
  demo_link: https://playground.inference-hub.com/models/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8

aim_recipes:
  - name: MI300X FP8
    hardware: MI300X
    precision: fp8
    recipe_file: configs/llama-4-maverick-17b-mi300x-fp8.yaml

  - name: MI250 FP8
    hardware: MI250
    precision: fp8
    recipe_file: configs/llama-4-maverick-17b-mi250-fp8.yaml

api_examples:
  python: |
    import requests

    headers = {
        "Authorization": "Bearer YOUR_API_KEY",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
        "messages": [{"role": "user", "content": "Hello"}],
        "stream": False
    }

    response = requests.post("https://api.inference-hub.com/v1/chat/completions", headers=headers, json=payload)
    print(response.json())

  javascript: |
    const response = await fetch("https://api.inference-hub.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Authorization": "Bearer YOUR_API_KEY",
        "Content-Type": "application/json"
      },
      body: JSON.stringify({
        model: "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
        messages: [{ role: "user", content: "Hello" }],
        stream: false
      })
    });

    const data = await response.json();
    console.log(data.choices[0].message.content);

  shell: |
    curl -X POST https://api.inference-hub.com/v1/chat/completions \
      -H "Authorization: Bearer YOUR_API_KEY" \
      -H "Content-Type: application/json" \
      -d '{
        "model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
        "messages": [{"role": "user", "content": "Hello"}],
        "stream": false
      }'

  java: |
    HttpClient client = HttpClient.newHttpClient();
    HttpRequest request = HttpRequest.newBuilder()
        .uri(URI.create("https://api.inference-hub.com/v1/chat/completions"))
        .header("Authorization", "Bearer YOUR_API_KEY")
        .header("Content-Type", "application/json")
        .POST(HttpRequest.BodyPublishers.ofString("""
          {
            "model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
            "messages": [{"role": "user", "content": "Hello"}],
            "stream": false
          }
        """))
        .build();

    HttpResponse<String> response = client.send(request, HttpResponse.BodyHandlers.ofString());
    System.out.println(response.body());

  go: |
    package main

    import (
        "bytes"
        "fmt"
        "io/ioutil"
        "net/http"
    )

    func main() {
        jsonStr := []byte(`{
            "model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
            "messages": [{"role": "user", "content": "Hello"}],
            "stream": false
        }`)

        req, _ := http.NewRequest("POST", "https://api.inference-hub.com/v1/chat/completions", bytes.NewBuffer(jsonStr))
        req.Header.Set("Authorization", "Bearer YOUR_API_KEY")
        req.Header.Set("Content-Type", "application/json")

        client := &http.Client{}
        resp, _ := client.Do(req)
        body, _ := ioutil.ReadAll(resp.Body)
        fmt.Println(string(body))
    }

  csharp: |
    using System.Net.Http;
    using System.Text;
    using System.Threading.Tasks;

    var client = new HttpClient();
    var request = new HttpRequestMessage(HttpMethod.Post, "https://api.inference-hub.com/v1/chat/completions");
    request.Headers.Add("Authorization", "Bearer YOUR_API_KEY");

    var json = """
    {
        "model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
        "messages": [{"role": "user", "content": "Hello"}],
        "stream": false
    }
    """;

    request.Content = new StringContent(json, Encoding.UTF8, "application/json");

    var response = await client.SendAsync(request);
    var responseBody = await response.Content.ReadAsStringAsync();
    Console.WriteLine(responseBody);

model_card:
  overview: >
    LLaMA 4 Maverick is a 17B parameter model with FP8 precision and 128K context length from Meta AI. 
    It's designed for high-performance inference with reduced memory usage through FP8 quantization 
    while maintaining excellent instruction-following capabilities and extended context processing.

  intended_use:
    - Long-context document processing
    - Conversational agents with extended memory
    - Document summarization and analysis
    - Retrieval-augmented generation (RAG)
    - Multilingual text generation
    - Code generation and analysis

  limitations:
    - May hallucinate facts
    - Not suitable for safety-critical use
    - FP8 precision may affect numerical accuracy
    - Performance may vary with very long sequences

  training_data: >
    Public web corpus, GitHub, Wikipedia, filtered Common Crawl, and other publicly available datasets.
    Training data cutoff: October 2023.

  evaluation:
    - "MMLU: 72.1"
    - "HumanEval (code): 48.3%"
    - "MT-Bench: 8.7"
    - "GSM8K: 82.1%"

  known_issues:
    - May produce biased or harmful content
    - FP8 quantization may affect certain numerical tasks
    - Performance varies across languages
    - Memory usage scales with context length

  references:
    - https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
    - https://github.com/facebookresearch/llama
    - https://ai.meta.com/llama/ 