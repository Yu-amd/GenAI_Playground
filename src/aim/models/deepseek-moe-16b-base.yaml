model_id: deepseek-ai/deepseek-moe-16b-base
name: DeepSeek MoE 16B Base
builder: DeepSeek AI
family: DeepSeek MoE
size: 16.4B
huggingface_id: deepseek-ai/deepseek-moe-16b-base
description: 'Mixture-of-Experts (MoE) language model with 16.4B parameters. It employs
  an innovative MoE architecture, which involves two principal strategies: fine-grained
  expert segmentation and shared experts isolation. This base model provides excellent
  efficiency and performance for various language tasks.

  '
logo: model_DeepSeek_MoE_18B.png
readiness_level: Day-0 Available
status_badges:
- FP16
- FlashAttention
- New
tags:
- MoE Architecture
- Efficient
- Base Model
- vLLM-Compatible
- Lightweight
license: Apache 2.0
endpoint: https://api.inference-hub.com/v1/chat/completions
demo_assets:
  notebook: https://github.com/inference-hub/notebooks/deepseek-moe-16b-base-demo.ipynb
  demo_link: https://playground.inference-hub.com/models/deepseek-ai/deepseek-moe-16b-base
aim_recipes:
- name: MI300X FP16
  hardware: MI300X
  precision: fp16
  recipe_file: configs/deepseek-moe-16b-base-mi300x-fp16.yaml
- name: MI250 FP16
  hardware: MI250
  precision: fp16
  recipe_file: configs/deepseek-moe-16b-base-mi250-fp16.yaml
api_examples:
  python: "import requests\n\nheaders = {\n    \"Authorization\": \"Bearer YOUR_API_KEY\"\
    ,\n    \"Content-Type\": \"application/json\"\n}\n\npayload = {\n    \"model\"\
    : \"deepseek-ai/deepseek-moe-16b-base\",\n    \"messages\": [{\"role\": \"user\"\
    , \"content\": \"Hello\"}],\n    \"stream\": False\n}\n\nresponse = requests.post(\"\
    https://api.inference-hub.com/v1/chat/completions\", headers=headers, json=payload)\n\
    print(response.json())\n"
  shell: "curl -X POST https://api.inference-hub.com/v1/chat/completions \\\n  -H\
    \ \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\"\
    \ \\\n  -d '{\n    \"model\": \"deepseek-ai/deepseek-moe-16b-base\",\n    \"messages\"\
    : [{\"role\": \"user\", \"content\": \"Hello\"}],\n    \"stream\": false\n  }'\n"
  go: "package main\n\nimport (\n    \"bytes\"\n    \"fmt\"\n    \"io/ioutil\"\n \
    \   \"net/http\"\n)\n\nfunc main() {\n    jsonStr := []byte(`{\n        \"model\"\
    : \"deepseek-ai/deepseek-moe-16b-base\",\n        \"messages\": [{\"role\": \"\
    user\", \"content\": \"Hello\"}],\n        \"stream\": false\n    }`)\n\n    req,\
    \ _ := http.NewRequest(\"POST\", \"https://api.inference-hub.com/v1/chat/completions\"\
    , bytes.NewBuffer(jsonStr))\n    req.Header.Set(\"Authorization\", \"Bearer YOUR_API_KEY\"\
    )\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client :=\
    \ &http.Client{}\n    resp, _ := client.Do(req)\n    body, _ := ioutil.ReadAll(resp.Body)\n\
    \    fmt.Println(string(body))\n}\n"
  typescript: "const response = await fetch(\"https://api.inference-hub.com/v1/chat/completions\"\
    , {\n  method: \"POST\",\n  headers: {\n    \"Authorization\": \"Bearer YOUR_API_KEY\"\
    ,\n    \"Content-Type\": \"application/json\"\n  },\n  body: JSON.stringify({\n\
    \    model: \"deepseek-ai/deepseek-moe-16b-base\",\n    messages: [{ role: \"\
    user\", content: \"Hello\" }],\n    stream: false\n  })\n});\n\nconst data = await\
    \ response.json();\nconsole.log(data.choices[0].message.content);\n"
  rust: "use axum::{\n    extract::Json,\n    http::StatusCode,\n    response::sse::{Event,\
    \ Sse},\n    routing::post,\n    Router,\n};\nuse serde::{Deserialize, Serialize};\n\
    use std::convert::Infallible;\nuse tokio_stream::wrappers::ReceiverStream;\n\n\
    #[derive(Deserialize)]\nstruct ChatRequest {\n    model: String,\n    messages:\
    \ Vec<Message>,\n    temperature: f32,\n    max_tokens: u32,\n    top_p: f32,\n\
    \    stream: bool,\n}\n\n#[derive(Serialize, Deserialize)]\nstruct Message {\n\
    \    role: String,\n    content: String,\n}\n\nasync fn chat_completion(Json(payload):\
    \ Json<ChatRequest>) -> Sse<impl Stream<Item = Result<Event, Infallible>>> {\n\
    \    let (tx, rx) = tokio::sync::mpsc::channel(100);\n    \n    tokio::spawn(async\
    \ move {\n        // Simulate streaming response\n        let response = format!(\"\
    Response for model: {}\", payload.model);\n        for chunk in response.chars()\
    \ {\n            let event = Event::default().data(chunk.to_string());\n     \
    \       let _ = tx.send(Ok(event)).await;\n            tokio::time::sleep(tokio::time::Duration::from_millis(50)).await;\n\
    \        }\n    });\n    \n    Sse::new(ReceiverStream::new(rx))\n}\n\n#[tokio::main]\n\
    async fn main() {\n    let app = Router::new()\n        .route(\"/chat/completions\"\
    , post(chat_completion));\n    \n    axum::Server::bind(&\"0.0.0.0:3000\".parse().unwrap())\n\
    \        .serve(app.into_make_service())\n        .await\n        .unwrap();\n\
    }"
model_card:
  overview: 'license_name: deepseek license_link: https://github.com/deepseek-ai/DeepSeek-MoE/blob/main/LICENSE-MODEL
    <img width="500px" alt="DeepSeek Chat" src="https://github.com/deepseek-ai/DeepSeek-LLM/blob/main/images/logo.png?raw=true">'
  intended_use:
  - Text-Generation tasks
  limitations:
  - May generate biased or harmful content
  - Not suitable for safety-critical applications
  - Performance may vary across different tasks and domains
  training_data: Training data information not specified in model card.
  evaluation:
  - Evaluation metrics not specified in model card
  known_issues:
  - May produce biased content
  - Limited reasoning capabilities
  - Performance varies across languages and domains
  references:
  - https://huggingface.co/deepseek-ai/deepseek-moe-16b-base
