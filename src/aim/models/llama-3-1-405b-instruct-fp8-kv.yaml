model_id: amd/Llama-3_1-405B-Instruct-FP8-KV
name: Llama 3.1 405B Instruct FP8 KV
builder: AMD
family: Llama
size: 405B
huggingface_id: amd/Llama-3_1-405B-Instruct-FP8-KV

description: >
  AMD's Llama-3.1-405B-Instruct-FP8-KV is a quantized version of Meta's Llama 3.1 405B Instruct model 
  using AMD's Quark framework. It applies FP8 quantization to weights, activations, and KV cache, 
  significantly reducing memory usage while maintaining high accuracy. The model uses symmetric per-tensor 
  quantization for optimal performance on AMD hardware.

logo: model_llama3_1_405b_fp8.png

readiness_level: Production-Ready
status_badges:
  - FP8
  - FlashAttention
  - Featured

tags:
  - Text Generation
  - Multilingual
  - Instruction-Tuned
  - vLLM-Compatible
  - Efficient

license: Meta RAIL

endpoint: https://api.inference-hub.com/v1/chat/completions

demo_assets:
  notebook: https://github.com/inference-hub/notebooks/llama-3-1-405b-fp8-kv-demo.ipynb
  demo_link: https://playground.inference-hub.com/models/amd/Llama-3.1-405B-Instruct-FP8-KV

aim_recipes:
  - name: MI300X FP8
    hardware: MI300X
    precision: fp8
    recipe_file: configs/llama-3-1-405b-fp8-kv-mi300x-fp8.yaml

  - name: MI250 FP8
    hardware: MI250
    precision: fp8
    recipe_file: configs/llama-3-1-405b-fp8-kv-mi250-fp8.yaml

api_examples:
  python: |
    import requests

    headers = {
        "Authorization": "Bearer YOUR_API_KEY",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "amd/Llama-3.1-405B-Instruct-FP8-KV",
        "messages": [{"role": "user", "content": "Hello"}],
        "stream": False
    }

    response = requests.post("https://api.inference-hub.com/v1/chat/completions", headers=headers, json=payload)
    print(response.json())

  javascript: |
    const response = await fetch("https://api.inference-hub.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Authorization": "Bearer YOUR_API_KEY",
        "Content-Type": "application/json"
      },
      body: JSON.stringify({
        model: "amd/Llama-3.1-405B-Instruct-FP8-KV",
        messages: [{ role: "user", content: "Hello" }],
        stream: false
      })
    });

    const data = await response.json();
    console.log(data.choices[0].message.content);

  shell: |
    curl -X POST https://api.inference-hub.com/v1/chat/completions \
      -H "Authorization: Bearer YOUR_API_KEY" \
      -H "Content-Type: application/json" \
      -d '{
        "model": "amd/Llama-3.1-405B-Instruct-FP8-KV",
        "messages": [{"role": "user", "content": "Hello"}],
        "stream": false
      }'

  java: |
    HttpClient client = HttpClient.newHttpClient();
    HttpRequest request = HttpRequest.newBuilder()
        .uri(URI.create("https://api.inference-hub.com/v1/chat/completions"))
        .header("Authorization", "Bearer YOUR_API_KEY")
        .header("Content-Type", "application/json")
        .POST(HttpRequest.BodyPublishers.ofString("""
          {
            "model": "amd/Llama-3.1-405B-Instruct-FP8-KV",
            "messages": [{"role": "user", "content": "Hello"}],
            "stream": false
          }
        """))
        .build();

    HttpResponse<String> response = client.send(request, HttpResponse.BodyHandlers.ofString());
    System.out.println(response.body());

  go: |
    package main

    import (
        "bytes"
        "fmt"
        "io/ioutil"
        "net/http"
    )

    func main() {
        jsonStr := []byte(`{
            "model": "amd/Llama-3.1-405B-Instruct-FP8-KV",
            "messages": [{"role": "user", "content": "Hello"}],
            "stream": false
        }`)

        req, _ := http.NewRequest("POST", "https://api.inference-hub.com/v1/chat/completions", bytes.NewBuffer(jsonStr))
        req.Header.Set("Authorization", "Bearer YOUR_API_KEY")
        req.Header.Set("Content-Type", "application/json")

        client := &http.Client{}
        resp, _ := client.Do(req)
        body, _ := ioutil.ReadAll(resp.Body)
        fmt.Println(string(body))
    }

  csharp: |
    using System.Net.Http;
    using System.Text;
    using System.Threading.Tasks;

    var client = new HttpClient();
    var request = new HttpRequestMessage(HttpMethod.Post, "https://api.inference-hub.com/v1/chat/completions");
    request.Headers.Add("Authorization", "Bearer YOUR_API_KEY");

    var json = """
    {
        "model": "amd/Llama-3.1-405B-Instruct-FP8-KV",
        "messages": [{"role": "user", "content": "Hello"}],
        "stream": false
    }
    """;

    request.Content = new StringContent(json, Encoding.UTF8, "application/json");

    var response = await client.SendAsync(request);
    var responseBody = await response.Content.ReadAsStringAsync();
    Console.WriteLine(responseBody);

model_card:
  overview: >
    AMD's Llama-3.1-405B-Instruct-FP8-KV is a quantized version of Meta's Llama 3.1 405B Instruct model 
    using AMD's Quark framework. It applies FP8 quantization to weights, activations, and KV cache, 
    significantly reducing memory usage while maintaining high accuracy. The model uses symmetric per-tensor 
    quantization for optimal performance on AMD hardware.

  intended_use:
    - High-performance text generation
    - Complex reasoning tasks
    - Large-scale language understanding
    - Research and development
    - Enterprise applications requiring efficiency

  limitations:
    - May hallucinate facts
    - Not suitable for safety-critical use
    - Requires significant computational resources
    - FP8 quantization may affect certain numerical tasks

  training_data: >
    Based on Meta's Llama 3.1 405B Instruct model training data. 
    Quantization applied using Pile dataset calibration samples.

  evaluation:
    - "Perplexity-wikitext2: 1.8951"
    - "Original PPL: 1.8561"
    - "Quantization loss: 2.1%"

  known_issues:
    - May produce biased content
    - FP8 quantization may affect certain numerical tasks
    - Performance varies across different domains
    - Requires AMD hardware for optimal performance

  references:
    - https://huggingface.co/amd/Llama-3.1-405B-Instruct-FP8-KV
    - https://github.com/AMD/Quark
    - https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct 