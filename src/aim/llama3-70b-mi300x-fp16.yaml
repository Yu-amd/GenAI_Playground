recipe_id: llama3-70b-mi300x-fp16
model_id: llama3-70b
huggingface_id: meta-llama/Meta-Llama-3-70B-Instruct
hardware: MI300X
precision: fp16

vllm_serve:
  "1_gpu":
    enabled: false

  "2_gpu":
    enabled: true
    args:
      --model: meta-llama/Meta-Llama-3-70B-Instruct
      --dtype: float16
      --tensor-parallel-size: 2
      --max-batch-size: 16
      --max-context-len: 8192
      --gpu-memory-utilization: 0.9
      --swap-space: 4GiB
      --trust-remote-code: true
      --port: 8000

  "4_gpu":
    enabled: true
    args:
      --model: meta-llama/Meta-Llama-3-70B-Instruct
      --dtype: float16
      --tensor-parallel-size: 4
      --max-batch-size: 32
      --max-context-len: 8192
      --gpu-memory-utilization: 0.9
      --swap-space: 4GiB
      --trust-remote-code: true
      --port: 8000

  "8_gpu":
    enabled: false

sglang_serve:
  "1_gpu":
    enabled: true
    args:
      --model: meta-llama/Meta-Llama-3-70B-Instruct
      --dtype: float16
      --max-batch-size: 8
      --gpu-memory-utilization: 0.9
      --trust-remote-code: true
      --port: 8001

  "2_gpu":
    enabled: true
    args:
      --model: meta-llama/Meta-Llama-3-70B-Instruct
      --dtype: float16
      --tensor-parallel-size: 2
      --max-batch-size: 16
      --gpu-memory-utilization: 0.9
      --trust-remote-code: true
      --port: 8001

  "4_gpu":
    enabled: true
    args:
      --model: meta-llama/Meta-Llama-3-70B-Instruct
      --dtype: float16
      --tensor-parallel-size: 4
      --max-batch-size: 32
      --gpu-memory-utilization: 0.9
      --trust-remote-code: true
      --port: 8001

  "8_gpu":
    enabled: false
